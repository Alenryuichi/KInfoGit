# AIæŠ€æœ¯å­¦ä¹ ç¬”è®° AI Learning Notes

## ğŸ“š å­¦ä¹ è·¯çº¿å›¾ Learning Roadmap

### åŸºç¡€ç†è®º Fundamentals
- [ ] **æœºå™¨å­¦ä¹ åŸºç¡€**
  - ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ 
  - æŸå¤±å‡½æ•°ã€ä¼˜åŒ–ç®—æ³•ã€æ­£åˆ™åŒ–
  - æ¨¡å‹è¯„ä¼°å’ŒéªŒè¯æ–¹æ³•

- [ ] **æ·±åº¦å­¦ä¹ æ ¸å¿ƒ**
  - ç¥ç»ç½‘ç»œåŸºç¡€ã€åå‘ä¼ æ’­
  - CNNã€RNNã€Transformeræ¶æ„
  - æ¿€æ´»å‡½æ•°ã€å½’ä¸€åŒ–æŠ€æœ¯

- [ ] **å¤§æ¨¡å‹åŸç†**
  - Transformeræ¶æ„æ·±åº¦è§£æ
  - æ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç 
  - é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥

### å·¥ç¨‹å®è·µ Engineering Practice
- [ ] **æ¨¡å‹éƒ¨ç½²**
  - ONNXã€TensorRTæ¨¡å‹ä¼˜åŒ–
  - æ¨¡å‹é‡åŒ–å’Œå‰ªææŠ€æœ¯
  - æ¨ç†æœåŠ¡æ¶æ„è®¾è®¡

- [ ] **æ¨ç†åŠ é€Ÿ**
  - GPUå¹¶è¡Œè®¡ç®—ä¼˜åŒ–
  - æ‰¹å¤„ç†å’Œæµæ°´çº¿æŠ€æœ¯
  - ç¼“å­˜å’Œé¢„è®¡ç®—ç­–ç•¥

- [ ] **AIåŸºç¡€è®¾æ–½**
  - æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
  - A/Bæµ‹è¯•å’Œç°åº¦å‘å¸ƒ
  - ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

## ğŸ› ï¸ æŠ€æœ¯æ ˆå­¦ä¹  Tech Stack Learning

### AIæ¡†æ¶ AI Frameworks
#### PyTorch
```python
# åŸºç¡€å¼ é‡æ“ä½œ
import torch
import torch.nn as nn

# ç®€å•ç¥ç»ç½‘ç»œç¤ºä¾‹
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

#### TensorFlow/Keras
```python
# Kerasæ¨¡å‹æ„å»º
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])
```

### æ¨ç†æœåŠ¡ Inference Services
#### FastAPI + PyTorch
```python
from fastapi import FastAPI
import torch
import uvicorn

app = FastAPI()
model = torch.load('model.pth')

@app.post("/predict")
async def predict(data: dict):
    # æ¨¡å‹æ¨ç†é€»è¾‘
    with torch.no_grad():
        result = model(torch.tensor(data['input']))
    return {"prediction": result.tolist()}
```

#### gRPCæœåŠ¡
```python
# AIæ¨ç†gRPCæœåŠ¡ç¤ºä¾‹
import grpc
from concurrent import futures
import inference_pb2_grpc

class InferenceService(inference_pb2_grpc.InferenceServicer):
    def __init__(self):
        self.model = load_model()
    
    def Predict(self, request, context):
        result = self.model.predict(request.data)
        return inference_pb2.PredictResponse(result=result)
```

## ğŸ—ï¸ AIç³»ç»Ÿæ¶æ„ AI System Architecture

### æ¨ç†æœåŠ¡æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   APIç½‘å…³   â”‚â”€â”€â”€â”€â”‚  è´Ÿè½½å‡è¡¡   â”‚â”€â”€â”€â”€â”‚  æ¨ç†æœåŠ¡   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   é™æµç»„ä»¶   â”‚    â”‚   æ’é˜Ÿç³»ç»Ÿ   â”‚    â”‚  æ¨¡å‹ç¼“å­˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ç»„ä»¶è®¾è®¡
#### 1. AIç½‘å…³ (AI Gateway)
- **åŠŸèƒ½**: è¯·æ±‚è·¯ç”±ã€è´Ÿè½½å‡è¡¡ã€é™æµç†”æ–­
- **æŠ€æœ¯**: Go + Redis + Consul
- **ç›‘æ§**: Prometheus + Grafana

#### 2. æ¨ç†æœåŠ¡ (Inference Service)
- **åŠŸèƒ½**: æ¨¡å‹åŠ è½½ã€æ¨ç†è®¡ç®—ã€ç»“æœè¿”å›
- **æŠ€æœ¯**: Python + PyTorch + CUDA
- **ä¼˜åŒ–**: æ¨¡å‹é‡åŒ–ã€æ‰¹å¤„ç†ã€GPUæ± åŒ–

#### 3. æ’é˜Ÿç³»ç»Ÿ (Queue System)
- **åŠŸèƒ½**: è¯·æ±‚æ’é˜Ÿã€ä¼˜å…ˆçº§è°ƒåº¦ã€èµ„æºç®¡ç†
- **æŠ€æœ¯**: Redis + Kafka + Go
- **ç­–ç•¥**: FIFOã€ä¼˜å…ˆçº§ã€å…¬å¹³è°ƒåº¦

## ğŸ“Š æ€§èƒ½ä¼˜åŒ– Performance Optimization

### æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯
#### é‡åŒ– (Quantization)
```python
# PyTorché‡åŒ–ç¤ºä¾‹
import torch.quantization as quantization

# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# é™æ€é‡åŒ–
model.qconfig = quantization.get_default_qconfig('fbgemm')
quantization.prepare(model, inplace=True)
# æ ¡å‡†æ•°æ®...
quantization.convert(model, inplace=True)
```

#### å‰ªæ (Pruning)
```python
import torch.nn.utils.prune as prune

# ç»“æ„åŒ–å‰ªæ
prune.ln_structured(
    module, name="weight", amount=0.3, n=2, dim=0
)

# éç»“æ„åŒ–å‰ªæ
prune.l1_unstructured(module, name="weight", amount=0.2)
```

### æ¨ç†åŠ é€Ÿç­–ç•¥
1. **æ‰¹å¤„ç†ä¼˜åŒ–**: åŠ¨æ€batchingï¼Œå‡å°‘GPUç©ºé—²
2. **æ¨¡å‹å¹¶è¡Œ**: å¤§æ¨¡å‹åˆ†ç‰‡éƒ¨ç½²
3. **æµæ°´çº¿å¹¶è¡Œ**: è®¡ç®—å’ŒIOé‡å 
4. **ç¼“å­˜ç­–ç•¥**: ç»“æœç¼“å­˜ã€ç‰¹å¾ç¼“å­˜

## ğŸ”§ å®è·µé¡¹ç›® Practice Projects

### é¡¹ç›®1: AIæ¨ç†æœåŠ¡å¹³å°
**ç›®æ ‡**: æ„å»ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„AIæ¨ç†æœåŠ¡
**æŠ€æœ¯æ ˆ**: Go + Python + Redis + Kubernetes
**åŠŸèƒ½ç‰¹æ€§**:
- [ ] å¤šæ¨¡å‹ç®¡ç†å’Œç‰ˆæœ¬æ§åˆ¶
- [ ] åŠ¨æ€è´Ÿè½½å‡è¡¡å’Œè‡ªåŠ¨æ‰©ç¼©å®¹
- [ ] è¯·æ±‚æ’é˜Ÿå’Œä¼˜å…ˆçº§è°ƒåº¦
- [ ] å®æ—¶ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ

### é¡¹ç›®2: å¤šæ¨¡æ€AIåº”ç”¨
**ç›®æ ‡**: å¼€å‘å›¾æ–‡ç†è§£å’Œç”Ÿæˆåº”ç”¨
**æŠ€æœ¯æ ˆ**: PyTorch + Transformers + FastAPI
**åŠŸèƒ½ç‰¹æ€§**:
- [ ] å›¾åƒç†è§£å’Œæè¿°ç”Ÿæˆ
- [ ] æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
- [ ] å¤šè½®å¯¹è¯å’Œä¸Šä¸‹æ–‡ç†è§£
- [ ] APIæœåŠ¡å’ŒWebç•Œé¢

### é¡¹ç›®3: AIç½‘å…³ç³»ç»Ÿ
**ç›®æ ‡**: å®ç°ä¼ä¸šçº§AIæœåŠ¡ç½‘å…³
**æŠ€æœ¯æ ˆ**: Go + etcd + Prometheus
**åŠŸèƒ½ç‰¹æ€§**:
- [ ] æœåŠ¡å‘ç°å’Œå¥åº·æ£€æŸ¥
- [ ] æµé‡æ§åˆ¶å’Œç†”æ–­é™çº§
- [ ] è®¤è¯æˆæƒå’Œå®‰å…¨é˜²æŠ¤
- [ ] ç›‘æ§æŒ‡æ ‡å’Œé“¾è·¯è¿½è¸ª

## ğŸ“– å­¦ä¹ èµ„æº Learning Resources

### åœ¨çº¿è¯¾ç¨‹
- [ ] **Stanford CS229**: Machine Learning
- [ ] **CS231n**: Convolutional Neural Networks
- [ ] **CS224n**: Natural Language Processing
- [ ] **Fast.ai**: Practical Deep Learning

### æŠ€æœ¯ä¹¦ç±
- [ ] ã€Šæ·±åº¦å­¦ä¹ ã€‹- Ian Goodfellow
- [ ] ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹- ææ²
- [ ] ã€Šæœºå™¨å­¦ä¹ ç³»ç»Ÿè®¾è®¡ã€‹- Chip Huyen
- [ ] ã€Šå¤§è§„æ¨¡æœºå™¨å­¦ä¹ ã€‹- å‘¨å¿—å

### å¼€æºé¡¹ç›®
- [ ] **Hugging Face Transformers**: é¢„è®­ç»ƒæ¨¡å‹åº“
- [ ] **Ray**: åˆ†å¸ƒå¼AIè®¡ç®—æ¡†æ¶
- [ ] **Triton**: NVIDIAæ¨ç†æœåŠ¡å™¨
- [ ] **BentoML**: æœºå™¨å­¦ä¹ æ¨¡å‹æœåŠ¡åŒ–

### æŠ€æœ¯åšå®¢å’Œè®ºæ–‡
- [ ] OpenAI Blog
- [ ] Google AI Blog  
- [ ] Meta AI Research
- [ ] arXiv.org AIè®ºæ–‡

## ğŸ¯ å­¦ä¹ è®¡åˆ’ Study Plan

### ç¬¬1ä¸ªæœˆ: åŸºç¡€ç†è®º
- Week 1-2: æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€
- Week 3-4: Transformeræ¶æ„å’Œå¤§æ¨¡å‹åŸç†

### ç¬¬2ä¸ªæœˆ: æ¡†æ¶å®è·µ
- Week 1-2: PyTorchæ·±åº¦å­¦ä¹ å®è·µ
- Week 3-4: æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒæŠ€æœ¯

### ç¬¬3ä¸ªæœˆ: å·¥ç¨‹åŒ–å®è·µ
- Week 1-2: æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†æœåŠ¡
- Week 3-4: æ€§èƒ½ä¼˜åŒ–å’Œç³»ç»Ÿè®¾è®¡

### ç¬¬4-6ä¸ªæœˆ: é¡¹ç›®å®æˆ˜
- å®Œæˆ3ä¸ªå®è·µé¡¹ç›®
- å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®
- æŠ€æœ¯åˆ†äº«å’Œæ€»ç»“

## ğŸ“ å­¦ä¹ ç¬”è®° Study Notes

### é‡è¦æ¦‚å¿µè®°å½•
- **æ³¨æ„åŠ›æœºåˆ¶**: Self-Attention, Multi-Head Attention
- **ä½ç½®ç¼–ç **: Absolute, Relative, RoPE
- **ä¼˜åŒ–ç®—æ³•**: Adam, AdamW, Lion
- **æ­£åˆ™åŒ–**: Dropout, LayerNorm, BatchNorm

### å®è·µç»éªŒæ€»ç»“
- æ¨¡å‹è®­ç»ƒçš„è°ƒå‚æŠ€å·§
- æ¨ç†æœåŠ¡çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•
- åˆ†å¸ƒå¼è®­ç»ƒçš„æœ€ä½³å®è·µ
- ç”Ÿäº§ç¯å¢ƒçš„ç›‘æ§å’Œè¿ç»´

---

*æœ€åæ›´æ–°: 2025å¹´8æœˆ19æ—¥*
*å­¦ä¹ è¿›åº¦: åŸºç¡€ç†è®ºé˜¶æ®µ*
